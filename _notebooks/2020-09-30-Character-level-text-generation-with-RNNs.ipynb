{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9myUIROC6p8k"
   },
   "source": [
    "# Character level text generation with RNNs using PyTorch Lightning\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [RNN, tutorial]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we will show how to generate the text using Recurrent Neural Networks. We will use it to generate surnames of people and while doing so we will take into account the country they come from. \n",
    "\n",
    "As a recurrent network, we will use LSTM. For the training, we will use PyTorch Lightning. We will show how to use the `collate_fn` so we can have batches of sequences of the different lengths. \n",
    "\n",
    "The article was inspired by https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html, the data used for the training was also taken from there"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# !pip install pytorch-lightning==0.9.1rc3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !pip install pytorch-lightning==0.9.1rc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !pip install pytorch-lightning==0.9.1rc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_Rp5LljYWog1"
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning==0.9.1rc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGhNKC1EXpb2"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WZn1V2YlWY-o"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRghOS81Ejy8"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "The Dataset consists of 20072 examples of surnames from different countries, data can be found [here](https://)\n",
    "\n",
    "\n",
    "We will use those examples to generate new names using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "v8Nk51mpWY-5",
    "outputId": "72d45681-c532-4e1f-a4a3-5cbd2f4b49ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>Abbas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>Abbey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>Abbott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>Abdi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>Abel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20069</th>\n",
       "      <td>Russian</td>\n",
       "      <td>Zolotnitsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20070</th>\n",
       "      <td>Russian</td>\n",
       "      <td>Zolotnitzky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071</th>\n",
       "      <td>Russian</td>\n",
       "      <td>Zozrov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20072</th>\n",
       "      <td>Russian</td>\n",
       "      <td>Zozulya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20073</th>\n",
       "      <td>Russian</td>\n",
       "      <td>Zukerman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20074 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category         Name\n",
       "0      English        Abbas\n",
       "1      English        Abbey\n",
       "2      English       Abbott\n",
       "3      English         Abdi\n",
       "4      English         Abel\n",
       "...        ...          ...\n",
       "20069  Russian  Zolotnitsky\n",
       "20070  Russian  Zolotnitzky\n",
       "20071  Russian       Zozrov\n",
       "20072  Russian      Zozulya\n",
       "20073  Russian     Zukerman\n",
       "\n",
       "[20074 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"text_generation/names.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "n424sHyxV6CD",
    "outputId": "ee031eb5-f27a-41a5-e9e4-5384afe9d057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Russian       9408\n",
       "English       3668\n",
       "Arabic        2000\n",
       "Japanese       991\n",
       "German         724\n",
       "Italian        709\n",
       "Czech          519\n",
       "Spanish        298\n",
       "Dutch          297\n",
       "French         277\n",
       "Chinese        268\n",
       "Irish          232\n",
       "Greek          203\n",
       "Polish         139\n",
       "Scottish       100\n",
       "Korean          94\n",
       "Portuguese      74\n",
       "Vietnamese      73\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceVgTYkYW6kY"
   },
   "source": [
    "# String to int and int to string mappers\n",
    "\n",
    "We will treat each letter as a separate element of a sequence. Thanks to that we don't need to perform any sophisticated tokenization, whatsoever. \n",
    "\n",
    "Besides letters, we will add tho additional tokens `<pad>` and `<eos>`. The fist is needed for padding (we will pad the names with 0 so we can have sequences of the same size in the single batch) while the second will be used to \"announce\" to the model that the name generation process ended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Xy1kxKmhWY_g",
    "outputId": "2f11390d-9994-4b95-e138-ae174fac511d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letters = [\"<pad>\"] + list(string.ascii_letters + \" .,;'-\") + [\"<eos>\"]\n",
    "n_letters = len(all_letters)\n",
    "n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ghIN-IINWY_0"
   },
   "outputs": [],
   "source": [
    "stoi = {letter : idx for idx, letter in enumerate(all_letters)}\n",
    "itos = [letter for idx, letter in enumerate(all_letters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-A-lUHwWWY_7",
    "outputId": "e6b79751-9f27-473d-a72c-9b031c7f8187"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, '<eos>')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi[\"<eos>\"], itos[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "U-5SF55EWZAC",
    "outputId": "64780814-836f-48fd-bfc0-9dcb5b00c363"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wg2h8iS9W_st"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "To provide the data to the model we need a `Dataset`. The one defined by us will return a dictionary of five elements. \n",
    "\n",
    "The three most important are: \n",
    "\n",
    "`category_tensor` - one-hot representation of one of the 18 categories.\n",
    "\n",
    "E.g English is represented as [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "`input_tensor` int representation of letter tokens from the name. \n",
    "\n",
    "`target_tensor` int representation of letter tokens from the target name. Target differs from input in such a way that it skips the first letter and adds `<eos>` at the end. \n",
    "\n",
    "We use 0 for `<pad>` token and 59 for `<eos>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xXgXUTtHWZAL"
   },
   "outputs": [],
   "source": [
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, df, stoi, eos_token=\"<eos>\"):\n",
    "        self.stoi = stoi\n",
    "        self.eos_token = eos_token\n",
    "        self.n_tokens = len(self.stoi)\n",
    "        \n",
    "        self.categories = df[\"Category\"].tolist()\n",
    "        self.names = df[\"Name\"].tolist()\n",
    "        \n",
    "        \n",
    "        self.all_categories = list(set(self.categories))\n",
    "        self.n_categories = len(self.all_categories)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        category = self.categories[item]\n",
    "        name = self.names[item]\n",
    "        \n",
    "        category_tensor = self.get_category_tensor(category)\n",
    "        \n",
    "        input_tensor = torch.tensor([stoi[char] for char in name])\n",
    "        target_tensor = torch.tensor([stoi[char] for char in list(name[1:])+[self.eos_token]])\n",
    "        \n",
    "        item_dict = {\"category\": category,\n",
    "        \"name\": name,\n",
    "        \"category_tensor\": category_tensor,\n",
    "        \"input_tensor\": input_tensor,\n",
    "        \"target_tensor\": target_tensor}\n",
    "        \n",
    "        \n",
    "        return item_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.categories)\n",
    "    \n",
    "    \n",
    "    def get_category_tensor(self, category):\n",
    "        li = self.all_categories.index(category)\n",
    "        tensor = torch.zeros(1, self.n_categories)\n",
    "        tensor[0][li] = 1\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "ql4VNiz6WZAS",
    "outputId": "ee049f4c-3f1d-478d-a4dc-8590c08ab874"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'English',\n",
       " 'category_tensor': tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'input_tensor': tensor([27,  2,  2,  1, 19]),\n",
       " 'name': 'Abbas',\n",
       " 'target_tensor': tensor([ 2,  2,  1, 19, 59])}"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = NamesDataset(df, stoi)\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6nyoFImXDm5"
   },
   "source": [
    "## Dataoader\n",
    "\n",
    "The Dataset returns sequences of a different length, which might cause problems, due to that we need to define the `collate_fn` method which will handle this issue. It will add padding (0) to at the end of the sequences that are shorter than the longest sentence in a batch. Thanks to that we can work with batches of size other than one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gjwFlBH_WZA4"
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        \"https://github.com/yunjey/seq2seq-dataloader/blob/master/data_loader.py\"\n",
    "        \n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        padded_seqs = torch.zeros(len(sequences), max(lengths)).long()\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq[:end]\n",
    "        return padded_seqs, lengths\n",
    "\n",
    "    categories = [x[\"category\"] for x in data]          \n",
    "    names = [x[\"name\"] for x in data]          \n",
    "    category_tensors = torch.cat([x[\"category_tensor\"] for x in data])\n",
    "    \n",
    "    input_tensors = [x[\"input_tensor\"] for x in data]\n",
    "    input_tensors, _ = merge(input_tensors)\n",
    "    \n",
    "    target_tensors = [x[\"target_tensor\"] for x in data]\n",
    "    target_tensors, _ = merge(target_tensors)\n",
    "    \n",
    "    return categories, names, category_tensors, input_tensors, target_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TCTBi0PwWZBB"
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=1, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "iNr4t8PjWZBH",
    "outputId": "03ce03fb-fdb9-49f9-dc0c-0bd6da3e4971"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Czech'],\n",
       " ['Lawa'],\n",
       " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[38,  1, 23,  1]]),\n",
       " tensor([[ 1, 23,  1, 59]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9iVBbolEy1n"
   },
   "source": [
    "# Lightning Datamodule\n",
    "\n",
    "To pass everything into the Lightning training loop we combine all of the previous steps and we define a `LightningDataModule` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7P4yq8MLWZBP"
   },
   "outputs": [],
   "source": [
    "class NamesDatamodule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.df = pd.read_csv(\"text_generation/names.csv\")\n",
    "        \n",
    "        self.all_letters = all_letters = [\"<pad>\"] + list(string.ascii_letters + \" .,;'-\") + [\"<eos>\"]\n",
    "        self.stoi = {letter : idx for idx, letter in enumerate(self.all_letters)}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_set = NamesDataset(df, self.stoi)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        def merge(sequences):\n",
    "            \"https://github.com/yunjey/seq2seq-dataloader/blob/master/data_loader.py\"\n",
    "\n",
    "            lengths = [len(seq) for seq in sequences]\n",
    "            padded_seqs = torch.zeros(len(sequences), max(lengths)).long()\n",
    "            for i, seq in enumerate(sequences):\n",
    "                end = lengths[i]\n",
    "                padded_seqs[i, :end] = seq[:end]\n",
    "            return padded_seqs, lengths\n",
    "\n",
    "        categories = [x[\"category\"] for x in data]          \n",
    "        names = [x[\"name\"] for x in data]          \n",
    "        category_tensors = torch.cat([x[\"category_tensor\"] for x in data])\n",
    "\n",
    "        input_tensors = [x[\"input_tensor\"] for x in data]\n",
    "        input_tensors, _ = merge(input_tensors)\n",
    "\n",
    "        target_tensors = [x[\"target_tensor\"] for x in data]\n",
    "        target_tensors, _ = merge(target_tensors)\n",
    "        \n",
    "        item_dict = {\"categories\": categories, \n",
    "                     \"names\": names, \n",
    "                     \"category_tensors\": category_tensors,\n",
    "                     \"input_tensors\": input_tensors,\n",
    "                     \"target_tensors\": target_tensors}\n",
    "\n",
    "        return item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9EJwLxLWZCG"
   },
   "source": [
    "## RNN Lightning\n",
    "\n",
    "We define a RNN. As a loss function, we will use `CrossEntropyLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZXGbi_uEWZCI"
   },
   "outputs": [],
   "source": [
    "class RNN(pl.LightningModule):\n",
    "    lr = 5e-4\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, embeding_size, n_categories, n_layers, output_size, p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.embeding = nn.Embedding(input_size+n_categories, embeding_size)\n",
    "        self.lstm = nn.LSTM(embeding_size+n_categories, hidden_size, n_layers, dropout=p)\n",
    "        self.out_fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "\n",
    "    def forward(self, batch_of_category, batch_of_letter, hidden, cell):\n",
    "        ## letter level operations\n",
    "        \n",
    "        embeding = self.dropout(self.embeding(batch_of_letter))\n",
    "        category_plus_letter = torch.cat((batch_of_category, embeding), 1)\n",
    "\n",
    "        #sequence_length = 1\n",
    "        category_plus_letter = category_plus_letter.unsqueeze(1)\n",
    "        \n",
    "        out, (hidden, cell) = self.lstm(category_plus_letter, (hidden, cell))\n",
    "        out = self.out_fc(out)\n",
    "        out = out.squeeze(1)\n",
    "        \n",
    "        return out, (hidden, cell)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), self.lr)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        item_dict = batch\n",
    "        loss = 0\n",
    "        batch_of_category = item_dict[\"category_tensors\"]\n",
    "\n",
    "        #to(device) needed due to some problem with PL\n",
    "        hidden = torch.zeros(self.n_layers, 1, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.n_layers, 1, self.hidden_size).to(self.device)\n",
    "\n",
    "        #we loop over letters, single batch at the time \n",
    "        for t in range(item_dict[\"input_tensors\"].size(1)):\n",
    "            batch_of_letter = item_dict[\"input_tensors\"][:, t]\n",
    "            \n",
    "            output, (hidden, cell) = self(batch_of_category, batch_of_letter, hidden, cell)\n",
    "            \n",
    "            loss += self.criterion(output, item_dict[\"target_tensors\"][:, t])\n",
    "\n",
    "        loss = loss/(t+1)\n",
    "\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFvTTSFG9ddN"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Finally, after defining the model and datamodule we can start training. For some strange reason that is not entirely clear to us, the model performs best when it is trained on a batch of size 1. After only 2 or 3 epochs it should be capable of generating the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292,
     "referenced_widgets": [
      "0f352de22b2f491fa7e62102069fae79",
      "cce593d10cfc4f9d88b196b292de24ee",
      "8639ca68674d467a9da3ca9ff82a2912",
      "d696800ddb344317b02fe53cd934cbe9",
      "e115d99adf8b46feb336c104fd95acda",
      "6c5fb200659b47e0af65381dc56c356a",
      "91e9b9eac8684ebcacc1f0309cc5e67c",
      "7c68d3cdacc747b6bbc9595b8ee37f92"
     ]
    },
    "id": "vS8UbRAMWZCO",
    "outputId": "9784d790-0caa-49d0-d464-bb4d7a2c8862"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | embeding  | Embedding        | 9 K   \n",
      "2 | lstm      | LSTM             | 940 K \n",
      "3 | out_fc    | Linear           | 15 K  \n",
      "4 | dropout   | Dropout          | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f352de22b2f491fa7e62102069fae79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = NamesDatamodule(1)\n",
    "\n",
    "rnn_model = RNN(input_size=ds.n_tokens,\n",
    "            hidden_size=256,\n",
    "            embeding_size = 128, \n",
    "            n_layers=2,    \n",
    "            n_categories=ds.n_categories,\n",
    "            output_size=ds.n_tokens,\n",
    "            p=0.3)\n",
    "\n",
    "\n",
    "trainer = Trainer(max_epochs=3, \n",
    "                  logger=None,\n",
    "                  gpus=1,\n",
    "                  early_stop_callback=False,\n",
    "                  checkpoint_callback=False,\n",
    "                  )\n",
    "\n",
    "trainer.fit(rnn_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTJJmnUnFJgH"
   },
   "source": [
    "# Generate names using the trained model\n",
    "\n",
    "Finally, we can check how our model handles the generation of names. It works quite well we can see that Russian names \"sound\" Russian as well as Japanese names \"sound\" Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qGq4hy7WWZCU"
   },
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(category, start_letter, model):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = ds.get_category_tensor(category)\n",
    "        \n",
    "        input = torch.tensor(ds.stoi[start_letter]).unsqueeze(0)\n",
    "        hidden, cell = model.init_hidden(1)\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, (hidden, cell) = model(category_tensor, input, hidden, cell)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == ds.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            else:\n",
    "                letter = itos[topi]\n",
    "                output_name += letter\n",
    "                \n",
    "            input = torch.tensor(ds.stoi[letter]).unsqueeze(0)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters, model):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "KtmyiXmxWZCa",
    "outputId": "6ede91b3-f8a9-4512-c05c-8089858b6c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allan\n",
      "Bowe\n",
      "Corris\n",
      "\n",
      "Abrakoff\n",
      "Babakov\n",
      "Cheparov\n",
      "\n",
      "Abata\n",
      "Bakata\n",
      "Chigawa\n"
     ]
    }
   ],
   "source": [
    "samples('English', 'ABC', rnn_model)\n",
    "print()\n",
    "samples('Russian', 'ABC', rnn_model)\n",
    "print()\n",
    "samples('Japanese', 'ABC', rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVIsXaU0FQqt"
   },
   "source": [
    "# Summary\n",
    "\n",
    "We showed how to define a LTSM network to generate text, our model works on a character level. We also showed how to define a dataloader for sequences of different lengths using `collate_fn`. \n",
    "\n",
    "Of course, the proposed network is very simple, but the goal of this article is to familiarize both author and the reader with the concept of Recurrent Neural Networks"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Character-Level-Text-Generation-with-RNNs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f352de22b2f491fa7e62102069fae79": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8639ca68674d467a9da3ca9ff82a2912",
       "IPY_MODEL_d696800ddb344317b02fe53cd934cbe9"
      ],
      "layout": "IPY_MODEL_cce593d10cfc4f9d88b196b292de24ee"
     }
    },
    "6c5fb200659b47e0af65381dc56c356a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c68d3cdacc747b6bbc9595b8ee37f92": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8639ca68674d467a9da3ca9ff82a2912": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch 2: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c5fb200659b47e0af65381dc56c356a",
      "max": 20074,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e115d99adf8b46feb336c104fd95acda",
      "value": 20074
     }
    },
    "91e9b9eac8684ebcacc1f0309cc5e67c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cce593d10cfc4f9d88b196b292de24ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "d696800ddb344317b02fe53cd934cbe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c68d3cdacc747b6bbc9595b8ee37f92",
      "placeholder": "​",
      "style": "IPY_MODEL_91e9b9eac8684ebcacc1f0309cc5e67c",
      "value": " 20074/20074 [05:33&lt;00:00, 60.22it/s, loss=1.589]"
     }
    },
    "e115d99adf8b46feb336c104fd95acda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}